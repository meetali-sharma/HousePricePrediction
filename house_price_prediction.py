# -*- coding: utf-8 -*-
"""House_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mXFcpx6qxdDGUz6hY8rn4xzza46KUmpE
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder,MinMaxScaler
from scipy.stats import skew,kurtosis
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor  # Import the DecisionTreeRegressor
from sklearn.model_selection import train_test_split

# Load the dataset
House_details = pd.read_csv('kc_house_data.csv')

House_details.head()       # to preview the first few rows To get a glimpse of the data structure, column names, and the type of values stored in the DataFrame.

# Convert date column to actual datetime format
House_details['date'] = pd.to_datetime(House_details['date'], errors='coerce')
House_details.head()

House_details.shape   # Use shape attribute to get number of rows and columns in the dataset

# Check for missing values if any
Missing_values = House_details.isnull().sum()
Missing_values

# Boxplot between categorical columns and target variable(price) to see how this features correlates with target.

# List of categorical columns
categorical_columns = ['id', 'waterfront', 'view', 'grade', 'condition', 'zipcode']

# Loop through each categorical column and plot a boxplot
for col in categorical_columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=col, y='price', data=House_details)
    plt.title(f'Box Plot: {col} vs price')
    plt.show()

# Label encoding categorical columns like condition, grade and zipcode

House_details['condition'] = LabelEncoder().fit_transform(House_details['condition'])
House_details['grade'] = LabelEncoder().fit_transform(House_details['grade'])
House_details['zipcode'] = LabelEncoder().fit_transform(House_details['zipcode'])

House_details = House_details.drop(columns=['id'])  # Exclude the column id from the dataset

# Calculated skewness and kurtosis to analyse the data distribution.

# Convert all columns to numeric if possible, otherwise replace with NaN
for col in House_details.columns:
    try:
        House_details[col] = pd.to_numeric(House_details[col])
    except ValueError:
        # Handle non-numeric columns by replacing with NaN
        House_details[col] = pd.to_numeric(House_details[col], errors='coerce')

# Calculate skewness and kurtosis, excluding NaN values
skewness = skew(House_details.select_dtypes(include=np.number).dropna(axis=1), nan_policy='omit') # Exclude columns with non-numeric values

print("Skewness of the data is:", skewness)

# Calculate kurtosis
kurt = kurtosis(House_details.select_dtypes(include=np.number).dropna(axis=1), fisher=True, nan_policy='omit')  # Exclude columns with non-numeric values
print("Kurtosis of the data is:", kurt + 3)  # Adding 3 to match the common definition

# Detect and remove outliers from data for better performance

numeric_columns = House_details.select_dtypes(include=np.number).columns # select numeric columns

Q1 = House_details[numeric_columns].quantile(0.25)
Q3 = House_details[numeric_columns].quantile(0.75)
IQR = Q3 - Q1

# Define outlier bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR


# Identify outliers based on the IQR
outliers = ((House_details[numeric_columns] < lower_bound) | (House_details[numeric_columns] > upper_bound)).any(axis=1)

# Count how many outliers are detected
num_outliers = outliers.sum()
print(f"Number of outliers detected using IQR method: {num_outliers}")

# Remove the outliers
outlier_free_data = House_details[~outliers]

# Show number of rows after outlier removal
print(f"Number of rows after removing outliers using IQR method: {outlier_free_data.shape[0]}")

# Initialize and apply MinMaxScaler Normalization
scaler = MinMaxScaler()
outlier_free_data[numeric_columns] = scaler.fit_transform(outlier_free_data[numeric_columns])

# Correlation matrix
corr_matrix = outlier_free_data.corr()

# Plot heatmap
plt.figure(figsize=(25, 15))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap between Numeric Columns and Price')
plt.show()

# Selecting features based on correlation with Price
corr_target = abs(corr_matrix['price'])
relevant_features = corr_target[corr_target > 0.2].index
final_data = outlier_free_data[relevant_features]

print(f"Selected Features based on correlation threshold:{relevant_features}")

print(final_data.shape[0] == outlier_free_data.shape[0]) #Ensure both dataframes align in terms of rows after removing outliers

# Train and Evaluate different models

if 'price' in relevant_features:
  relevant_features = relevant_features.drop('price')

final_data = outlier_free_data[relevant_features]

# Train linear regression model by using relevant features

X = final_data  # Features (all columns in final_data)
y = outlier_free_data['price']  # Target variable (price column)


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Calculate regression metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Print the metrics
print(f"Mean Absolute Error: {mae:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared (R2): {r2:.2f}")

# Plotting y_test vs y_pred
plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred, label='Actual vs. Predicted') # Now scatter plot uses `y_test` as `x` and `y_pred` as `y`
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label='Perfect Prediction') # This line represents perfect prediction
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs. Predicted Prices')
plt.legend()
plt.show()

# Train and Evaluate Linear Regression model on the features selected using domain knowledge

features= ['date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',
           'floors', 'waterfront', 'view', 'condition', 'grade','yr_built', 'lat', 'long'
        ]

X = outlier_free_data[features]  # Features
y = outlier_free_data['price']  # Target variable (price column)


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Calculate regression metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Print the metrics
print(f"Mean Absolute Error: {mae:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared (R2): {r2:.2f}")

# Plotting y_test vs y_pred
plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred, label='Actual vs. Predicted') # Now scatter plot uses `y_test` as `x` and `y_pred` as `y`
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label='Perfect Prediction') # This line represents perfect prediction
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs. Predicted Prices')
plt.legend()
plt.show()

# Train and Evaluate Random Forest model

X = outlier_free_data[features]  # Features
y = outlier_free_data['price']  # Target variable (price column)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Random Forest regressor
RF_model = RandomForestRegressor(n_estimators=3, random_state=42)
RF_model.fit(X_train, y_train)

# Make predictions
y_pred = RF_model.predict(X_test)

# Calculate regression metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Print the metrics
print(f"Mean Absolute Error: {mae:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared (R2): {r2:.2f}")

# Plotting y_test vs y_pred
plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred, label='Actual vs. Predicted') # Now scatter plot uses `y_test` as `x` and `y_pred` as `y`
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label='Perfect Prediction') # This line represents perfect prediction
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs. Predicted Prices')
plt.legend()
plt.show()

# Train and Evaluate AdaBoost model with Decision Tree as base estimator

X = outlier_free_data[features]  # Features
y = outlier_free_data['price']  # Target variable (price column)


# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create an AdaBoost classifier with a decision tree base estimator
base_estimator = DecisionTreeRegressor(max_depth=2)  # You can specify the depth of the tree
DT_model = AdaBoostRegressor(estimator=base_estimator, n_estimators=7, random_state=42)
DT_model.fit(X_train, y_train)

# Make predictions
y_pred = DT_model.predict(X_test)

# Calculate regression metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Print the metrics
print(f"Mean Absolute Error: {mae:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared (R2): {r2:.2f}")

# Plotting y_test vs y_pred
plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred, label='Actual vs. Predicted') # Now scatter plot uses `y_test` as `x` and `y_pred` as `y`
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label='Perfect Prediction') # This line represents perfect prediction
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs. Predicted Prices')
plt.legend()
plt.show()

# Train and Evaluate Gradient Boosting model

X = outlier_free_data[features]  # Features
y = outlier_free_data['price']  # Target variable (price column)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a GradientBoostingClassifier
GB_model= GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=42)
GB_model.fit(X_train, y_train)

# Make predictions
y_pred = GB_model.predict(X_test)

# Calculate regression metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Print the metrics
print(f"Mean Absolute Error: {mae:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared (R2): {r2:.2f}")

# Plotting y_test vs y_pred
plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred, label='Actual vs. Predicted') # Now scatter plot uses `y_test` as `x` and `y_pred` as `y`
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label='Perfect Prediction') # This line represents perfect prediction
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs. Predicted Prices')
plt.legend()
plt.show()